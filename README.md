# 3DGaussianSplatting : Create a 3D Gaussian Splatting of a scene

## üéØ Project Overview
In this project, we are implementing a **visual computing pipeline** to reconstruct a 3D scene from a multi-view image sequence using **3D Gaussian Splatting**.

The system will transform 2D image frames into a **real-time renderable radiance field**, enabling smooth, interactive viewpoint navigation.

We are achieving this through these high level steps:

1. **Structure-from-Motion (SfM)** using **COLMAP**  
   ‚Üí Recovers **camera poses** + **sparse 3D point cloud**

2. **Gaussian Splatting Initialization**  
   ‚Üí Converts COLMAP reconstruction into anisotropic 3D Gaussians

3. **Gaussian Optimization + Rendering**  
   ‚Üí Produces a photorealistic, real-time view-synthesis model

This work follows the method introduced in:  (base research paper)

> **Kerbl, Bernhard, et al.**  
> *"3D Gaussian Splatting for Real-Time Radiance Field Rendering."*  
> **ACM TOG 42.4 (2023).**

---

## üöÄ Project Layers

To de-risk development we split the work into five incremental layers:

‚Ä¢ **Functional Minimum**: Generate a 3D point cloud from multi-view images using COLMAP
and visualize it.

‚Ä¢ **Minimum Goal**: Convert COLMAP results to Gaussian representation and render a basic
static scene.

‚Ä¢ **Desired Goal**: Achieve real-time rendering with adjustable camera viewpoints.

‚Ä¢ **Maximum Goal**: Add basic user controls (camera orbit, zoom, reset) or improve the
rendering pipeline with automated parameter setup.

‚Ä¢ **Extras**: Automate the full pipeline (video ‚Üí frames ‚Üí COLMAP ‚Üí Gaussian ‚Üí render).

This project demonstrates applied understanding of **computer vision, 3D geometry, GPU optimization, and rendering**.

---

# üìÇ Environment Setup & Pipeline Steps

Below is the exact pipeline followed by us to reach the goal of our project

---

## 1. System level environment setup

We are using MacOS as our base OS, so we first installed **Homebrew** (macOS package manager)  
  Install command used:
  ```bash
  /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
  # then added brew to PATH:
  echo 'eval "$(/opt/homebrew/bin/brew shellenv)"' >> ~/.zprofile
  eval "$(/opt/homebrew/bin/brew shellenv)"
  ```
Then we installed the required tools using brew commands : 
```bash
brew install ffmpeg
brew install colmap
```
After this, we created the python virtual environment and installed the basic libraries which we were going to use in our project : (used PyCharm Community edition)
```bash
python3 -m venv gs_env
source gs_env/bin/activate
pip install --upgrade pip
```
Installing libraries through pycharm terminal:
```bash
pip install torch torchvision torchaudio      # PyTorch (Apple MPS locally)
pip install open3d numpy matplotlib opencv-python pillow tqdm
pip install jupyterlab 
```
 ### Project File Structure:
 ```bash
GaussianSplatting_Project/
 ‚îú‚îÄ data/
 ‚îÇ   ‚îú‚îÄ raw/                     # original video(s)
 ‚îÇ   ‚îú‚îÄ frames/test/             # extracted frames used for COLMAP
 ‚îÇ   ‚îî‚îÄ colmap_output/           # COLMAP outputs (database, sparse, etc.)
 ‚îú‚îÄ scripts/
 ‚îÇ   ‚îú‚îÄ frame_extraction.py
 ‚îÇ   ‚îú‚îÄ remove_blurry_frames.py
 ‚îÇ   ‚îî‚îÄ visualize_colmap_sparse.py
 ‚îú‚îÄ results/
 ‚îî‚îÄ README.md
```

## 2. Collecting dataset and frame extraction

We have used a short video of a well-lighted outside setting near as our initial testing dataset(if we get our desired result, we are planning to use a better video with higher fps) We used Iphone with 30fps to shoot the video and put it in our data>>raw>> folder to further extract frames from it.

### Frame Extraction:
We are using cv2 library to extract frames from our video. For now the extraction rate is 5 frames per second (changeable as per requirement).
```bash
python scripts/frame_extraction.py
```
After running this script the output images can be stored in our frames folder under data.

### Blurry Frame Removal (Variance of Laplacian Method):
Before sending our frames into COLMAP, we cleaned them by removing the blurry ones. We used a simple sharpness check based on the variance of the Laplacian, which basically measures how many edges or details an image has. Sharp images have strong edges, while blurry images have very weak ones. So in the script, we loop through all frames, calculate this sharpness score, and then decide which images are ‚Äútoo blurry‚Äù by comparing them to an adaptive threshold based on the median sharpness of the whole set. Any frame below that threshold gets moved into a separate folder. This way, we make sure COLMAP only receives good, sharp frames, which improves the feature matching and the final reconstruction quality.
```bash
python scripts/remove_blurry_frames.py
```

## 3. Feature Extraction and Feature Matching

Once we had our cleaned frames, we used COLMAP to detect SIFT features in every image.
These features act like ‚Äúlandmarks‚Äù that COLMAP can track across multiple frames.
We stored everything in database.db, which COLMAP uses in the next steps.
```bash
colmap feature_extractor \
    --database_path data/colmap_output/database.db \
    --image_path data/frames/test \
    --ImageReader.single_camera 1
```
After extracting features, we told COLMAP to match them between all image pairs.
This helps COLMAP figure out which frames overlap and which points correspond to each other in 3D space.
Matching took some time, but it was a key step because without this, COLMAP can‚Äôt reconstruct the scene.
```bash
colmap exhaustive_matcher --database_path data/colmap_output/database.db
```

## 4. Sparse Reconstruction (Mapper)

With features and matches ready, we ran COLMAP‚Äôs mapper.
This is where COLMAP actually estimates:
-the camera poses for each frame
-a sparse 3D point cloud of the scene

This is basically COLMAP ‚Äúsolving‚Äù the geometry of the scene.
The output is written into data/colmap_output/sparse/, specifically inside the folder 0/.

```bash
colmap mapper \
    --database_path data/colmap_output/database.db \
    --image_path data/frames/test \
    --output_path data/colmap_output/sparse
```
After this we will get these files as output:
```python
cameras.bin
images.bin
points3D.bin
```

## 5. Exporting the Sparse Model to PLY

COLMAP stores reconstructions in its own binary format, so to visualize them easily,
we exported the 3D points into a .ply file.
We had to give COLMAP a full output filepath, otherwise macOS throws an error.
This generated points3D.ply, which is basically our sparse point cloud.

```bash
colmap model_converter \
    --input_path data/colmap_output/sparse/0 \
    --output_path data/colmap_output/sparse/0/points3D.ply \
    --output_type PLY
```

## 6. Visualizing the Sparse Point Cloud

We loaded the exported .ply file in a small Open3D script to see what the reconstruction looked like.

```python
import open3d as o3d
import os

# path to your exported point cloud
ply_path = "../data/colmap_output/sparse/0/points3D.ply"

if not os.path.exists(ply_path):
    print(f" File not found: {ply_path}")
else:
    print(f" Loading point cloud from: {ply_path}")
    pcd = o3d.io.read_point_cloud(ply_path)
    print(pcd)  # prints number of points etc.
    o3d.visualization.draw_geometries(
        [pcd],
        window_name="COLMAP Sparse Reconstruction",
        width=1000,
        height=800,
    )
```
The visualization usually shows: a tight cluster of points, sometimes shifted away from the center or sometimes not directly recognizable.
This is normal because COLMAP only reconstructs distinctive feature points and doesn‚Äôt densify the scene.

<img width="493" height="430" alt="Screenshot 2025-12-09 at 10 57 57‚ÄØAM" src="https://github.com/user-attachments/assets/0926ee16-d95c-4e0c-b8df-b23df3e26eee" />

---

## 7. Image Undistortion

After finishing the sparse reconstruction, we needed to undistort the frames because Gaussian Splatting only supports PINHOLE or SIMPLE_PINHOLE intrinsics.
COLMAP‚Äôs default SfM uses more complex models (e.g., OPENCV), so we fixed this locally.

We ran the COLMAP undistorter on our machine:

```bash
colmap image_undistorter \
    --image_path data/frames/test \
    --input_path data/colmap_output/sparse/0 \
    --output_path scene_gs \
    --output_type COLMAP
```
This created a new folder scene_gs/ with:
```python
scene_gs/
‚îÇ‚îÄ‚îÄ images/          (undistorted images)
‚îÇ‚îÄ‚îÄ sparse/
‚îÇ     ‚îú‚îÄ‚îÄ cameras.bin
‚îÇ     ‚îú‚îÄ‚îÄ images.bin
‚îÇ     ‚îî‚îÄ‚îÄ points3D.bin
```

### Converting COLMAP Binary Files (.bin ‚Üí .txt)
Gaussian Splatting does not read .bin files, so we converted everything to text format and saved it to 0 folder in sparse.
```bash
colmap model_converter \
    --input_path scene_gs/sparse \
    --output_path scene_gs/sparse/0 \
    --output_type TXT
```
Final dataset structure:
```bash
scene_gs/
‚îÇ‚îÄ‚îÄ images/
‚îÇ‚îÄ‚îÄ sparse/
‚îÇ     ‚îî‚îÄ‚îÄ 0/
‚îÇ          ‚îú‚îÄ‚îÄ cameras.txt
‚îÇ          ‚îú‚îÄ‚îÄ images.txt
‚îÇ          ‚îî‚îÄ‚îÄ points3D.txt
```

## 8. Training on the GPU Server (University Cluster)
Since our local machine (Mac M2) cannot train Gaussian Splatting models, we moved the final training pipeline to the university GPU server. Below is everything we did to get training running there.

### A. Connect to the server
After connecting to the university VPN, we accessed the GPU VM using VS Code Remote. The project workspace on the server is located at /workspace/.
Inside this directory, we keep:

-the official gaussian-splatting repository

-our own gaussian_project folder (dataset + outputs)

### B. Clone the official Gaussian Splatting repository
We cloned the upstream repository as we were not able to use the CLI commands for covert,train and render also because it contains:

-the training script (train.py)

-the renderer

-the COLMAP scene loader

-CUDA-based acceleration modules needed for real-time GS

```bash
cd /workspace
git clone https://github.com/graphdeco-inria/gaussian-splatting.git
```
This will give us full-training framework.

### C. Uploaded our prepared dataset (scene_gs) to the server
We first prepared the dataset locally (undistortion + bin‚Üítxt conversion) and then uploaded the entire folder to ""/workspace/gaussian_project/scene_gs""
Final dataset structure on the server:
```bash
scene_gs/
 ‚îú‚îÄ‚îÄ images/               # undistorted frames
 ‚îú‚îÄ‚îÄ sparse/
 ‚îÇ     ‚îî‚îÄ‚îÄ 0/
 ‚îÇ         ‚îú‚îÄ‚îÄ cameras.txt
 ‚îÇ         ‚îú‚îÄ‚îÄ images.txt
 ‚îÇ         ‚îú‚îÄ‚îÄ points3D.txt
 ‚îî‚îÄ‚îÄ (other COLMAP files)
```
This is exactly the format expected by the Gaussian Splatting training pipeline.

### D. Build the CUDA extensions required for Gaussian Splatting
The training code depends on three custom CUDA modules:

-diff-gaussian-rasterization

-simple-knn

-fused-ssim

These cannot be installed via pip; they must be compiled against the server‚Äôs CUDA + PyTorch setup.

For each module:
```bash
cd gaussian-splatting/submodules/<module-name>
/home/student/venv/bin/python setup.py build_ext --inplace
```
We repeated this for all three of them.

### E. Install missing Python packages inside the server's venv
The server had already preinstalled:

a) CUDA-enabled PyTorch

b) the venv (/home/student/venv)

c) GPU drivers

So we only needed to add these missing small packages: 
```bash
/home/student/venv/bin/pip install tqdm plyfile opencv-python joblib
```

### F. Run the Gaussian Splatting training
Once the dataset and CUDA modules were ready, we launched training:
```bash
cd /workspace/gaussian-splatting
/home/student/venv/bin/python train.py \
    -s /workspace/gaussian_project/scene_gs \
    -m /workspace/gaussian_project/output_model \
    --iterations 15000
```
This:

-loads the COLMAP scene

-initializes 3D Gaussians

-optimizes them for 15000 iterations

-stores outputs under "/workspace/gaussian_project/output_model/"

Inside this folder, the renderer also creates two subfolders:
```bash
output_model/train/
output_model/test/
```
Each subfolder contains a directory named ours_15000, which holds the rendered frames generated from the trained model.

## 9. Rendering Final Images Using the Trained Model
After training finished, we used render.py to generate the 2D renderings from the optimized 3D Gaussians.
To render the training split:
```bash
/home/student/venv/bin/python render.py \
    -m /workspace/gaussian_project/output_model \
    -s /workspace/gaussian_project/scene_gs \
    --train
```
This produced the final RGB images at:
```swift
/workspace/gaussian_project/output_model/train/ours_15000/renders/
```
These are the frames that we later assembled into a video.

## 10. Converting Rendered Frames into a Video
The frames produced by Gaussian Splatting are simply PNG images.
We used FFmpeg to turn them into an MP4 video.

Important: some images had odd dimensions, which FFmpeg does not accept for H.264 encoding, so we padded the height by 1 pixel.

From inside the directory containing the PNG files:
```bash
ffmpeg -framerate 30 -pattern_type glob -i "*.png" \
    -vf "pad=width=iw:height=ih+1" \
    -c:v libx264 -pix_fmt yuv420p output_15000.mp4
```
This produced our final rendered video **output_15000.mp4**. The generated PNG frames (and the resulting MP4 video) are not just the original video.
Instead, they represent:

-A re-rendering of the scene

-Using only the learned 3D Gaussians

-From the same camera poses that COLMAP estimated

-After 15,000 iterations of optimization

This verifies that:

-The pipeline successfully reconstructed the scene in 3D

-The Gaussian model can render synthetic views that match the input images

-The system achieves the Minimum Goal and part of the Desired Goal

## 11. Rerun everything on better dataset with more training iterations
After we verified that the training and rendering are working as expected. We introduced our new data set which is a video taken near our campus's library in high resolution and re-ran the whole pipeline till rendering.
And we tested the training on different number of iterations and placed checkpoints on 7000, 10000, 15000, 40000 iterations. We observed that after the 40k iterations the progress got stagnant hence we stopped training the model on 40k. 
The point cloud of splats we got was better in resolution than the last one.

After that we created the video from rendered images just like before, this shows that the scene got reconstructed well. Although the video which was generated was not very aesthetically pleasing because of the different intrinsics of mobile phone camera and colmap, but we can see that the gaussian splatting training worked well.
To visualize the 3d scene better we chose Novel View Synthesis (Orbit & Spiral Rendering).

## 12. Novel View Synthesis (Orbit & Spiral Rendering)
After verifying that the trained Gaussian model could correctly re-render the training camera views, we extended the pipeline to demonstrate novel view synthesis.

Novel view synthesis means rendering the learned 3D scene from camera viewpoints that were never present in the original dataset. This is one of the core promises of radiance-field methods like Gaussian Splatting.

Instead of using COLMAP‚Äôs real camera poses, we generate synthetic camera trajectories programmatically and render the scene from those viewpoints.

This step moves the project beyond simple reconstruction and shows that the model has learned a continuous 3D representation of the scene.
We created two new python scripts within the official gaussian-splatting repo - "render_orbit.py","render_spiral.py" which :
-loaded the trained Gaussian model from
/workspace/gaussian_project/output_model/

-reused the trained 3D Gaussians without retraining

-replaced COLMAP camera poses with synthetic camera poses

-rendered one PNG frame per synthetic viewpoint
The rendered frames were written to:
```bash
/workspace/gaussian_project/results/novel_orbit/
(workspace/gaussian_project/results/novel_spiral/)
```
Each folder contains:

orbit_0000.png ... orbit_0239.png
or

spiral_0000.png ... spiral_0239.png

These frames were then converted into MP4 videos using FFmpeg, exactly like the training renders.
We used these 2 type camera path for rendering:
### A. Orbit Camera Path
In orbit rendering, the camera:

-moves on a circular path around the scene center

-keeps looking toward the center of the scene

-maintains fixed intrinsics

This produced a smooth 360¬∞ rotation around the scene and in the video conceptually, radius is fixed, angle changes uniformly & height remains constant.

### B. Spiral Camera Path
In spiral rendering, the camera:

-rotates around the scene (like orbit)

-changes radius over time

-changes height over time

This produced a helical or spiral motion that moves closer/farther from the scene, moves slightly up/down & continuously changes viewpoint.

## 13. Why Orbit & Spiral Videos Look Different from Training Renders
It is normal for orbit and spiral renders to look slightly different from training renders.
These differences do not indicate a failure of the model.
Instead, they confirm that the model is performing novel view synthesis, which was our intended goal.

### Training Renders
-Use camera poses from the original dataset

-Are directly optimized during training

-Typically appear sharper and more detailed

### Orbit and Spiral Renders
-Use new, unseen camera poses

-Require interpolation between learned Gaussians

-May reveal softer details or minor artifacts

## 14. Final Verdict
The primary goal of this project was to achieve novel view synthesis using a 3D Gaussian Splatting pipeline, and this goal was successfully accomplished. Starting from a raw phone video, we processed the data through frame extraction and Structure-from-Motion to obtain camera poses and scene geometry. Using this information, we trained a Gaussian-based representation capable of rendering stable and realistic novel views. Orbit and spiral camera paths were used to evaluate the reconstruction, and the resulting renders showed consistent geometry, smooth parallax, and visually coherent views from unseen camera positions. This confirms that the model learned a meaningful 3D structure rather than simply reproducing the training images.

We also visualized the reconstructed scene in Unity and added basic user controls. Features like camera orbit and zoom made it easy to explore the orbit and spiral views. Although Unity was not the main focus, it provided a useful way to view the results of the novel view synthesis pipeline. In summary, the project met its main goal, and the interactive features show promise for real-time use and future improvements.
